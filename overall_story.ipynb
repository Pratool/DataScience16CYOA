{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Story\n",
    "#### Pratool Gadtaula and Jay Woo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every day, tons of reviews are posted on Rotten Tomatoes, ranging from glowingly positive to vehemently negative. Often, it is useful for a computer to calculate sentiment automatically, so that a human doesn't have to sift through all the data manually. For the most part, this problem can be solved by feeding the data into a natural language processing model that calculates the sentiment of new reviews. However, there is a lot of nuance in the English language, and a lot of different factors need to be taken into consideration.\n",
    "\n",
    "The Kaggle competition we downloaded the data from can be found here: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews, and our approach to this problem is detailed below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooj/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import pattern.en\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t')\n",
    "test = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major property of language is part of speech. After watching Coursera lectures on NLP (https://class.coursera.org/nlp/lecture) we decided to tag the part of speech of each word in the data using the averaged perceptron tagger provided by NLTK (https://spacy.io/blog/part-of-speech-POS-tagger-in-python). However, it is important to note that taggers are not very good at handling sentence ambiguity (“Republicans grill IRS chief over lost emails”), thus yielding an accuracy of around 57% at best (http://nlp.stanford.edu/pubs/CICLing2011-manning-tagging.pdf). The NLTK-provided tagger has many different parts of speech, so in order to prevent the model from overfitting, we only used 4 categories: adjectives, adverbs, verbs, and nouns (inspired by http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212278). For our approach, we assumed a bag of words model, which essentially says that it doesn't matter what order everything is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the tagger for faster tagging\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def tag_all_phrases(df):\n",
    "    data = df\n",
    "    data[\"POS\"] = data[\"Phrase\"].apply(\n",
    "        lambda x: [tag[1] for tag in \\\n",
    "                   nltk.tag._pos_tag(nltk.word_tokenize(x), None, tagger)] )\n",
    "    return data\n",
    "\n",
    "data_pos = tag_all_phrases(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating</td>\n",
       "      <td>2</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>adage</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>what</td>\n",
       "      <td>2</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId         Phrase  Sentiment  POS\n",
       "3          4           1              a          2   DT\n",
       "4          5           1         series          2   NN\n",
       "6          7           1             of          2   IN\n",
       "8          9           1      escapades          2  NNS\n",
       "11        12           1  demonstrating          2  VBG\n",
       "13        14           1            the          2   DT\n",
       "14        15           1          adage          2   NN\n",
       "16        17           1           that          2   IN\n",
       "18        19           1           what          2   WP\n",
       "20        21           1             is          2  VBZ"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words(df):\n",
    "    data = df[~(df['Phrase'].str.contains(' '))]\n",
    "    data = data[data.Phrase.str.contains('^[a-zA-Z]+$')]\n",
    "    data.Phrase = data.Phrase.str.lower()\n",
    "    return data\n",
    "\n",
    "def extract_single_POS_from_words(df):\n",
    "    data = df\n",
    "    data[\"POS\"] = data[\"POS\"].apply(lambda x: x[0])\n",
    "    return data\n",
    "\n",
    "words_pos = get_words(data_pos)\n",
    "words_pos = extract_single_POS_from_words(words_pos)\n",
    "words_pos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNS', 'PRP', 'PRP$', 'RB', 'RBR', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n"
     ]
    }
   ],
   "source": [
    "# Prints all of the unique parts of speech that have been tagged\n",
    "print sorted(words_pos['POS'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags in the \"POS\" column represent different parts of speech, and the definitions for each abbreviation can be found here: <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">part of speech tags used by the Penn Treebank</a>. Now that we have these parts of speech, we can separate each word into 5 categories - noun, adjective, adverb, verb, or miscellaneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>DT</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "      <td>IN</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "      <td>NNS</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating</td>\n",
       "      <td>2</td>\n",
       "      <td>VBG</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId         Phrase  Sentiment  POS  POS2\n",
       "3          4           1              a          2   DT  misc\n",
       "4          5           1         series          2   NN  noun\n",
       "6          7           1             of          2   IN  misc\n",
       "8          9           1      escapades          2  NNS  noun\n",
       "11        12           1  demonstrating          2  VBG  verb"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_general_POS(data):\n",
    "    pos_map = {\n",
    "        \"NNS\":\"noun\",\n",
    "        \"NN\":\"noun\",\n",
    "        \"NNP\":\"noun\",\n",
    "        \"NNPS\":\"noun\",\n",
    "        \"RB\":\"adverb\",\n",
    "        \"RBR\":\"adverb\",\n",
    "        \"RBS\":\"adverb\",\n",
    "        \"VB\":\"verb\",\n",
    "        \"VBD\":\"verb\",\n",
    "        \"VBG\":\"verb\",\n",
    "        \"VBN\":\"verb\",\n",
    "        \"VBP\":\"verb\",\n",
    "        \"VBZ\":\"verb\",\n",
    "        \"JJ\":\"adjective\",\n",
    "        \"JJR\":\"adjective\",\n",
    "        \"JJS\":\"adjective\"\n",
    "    }\n",
    "    if data not in pos_map: return \"misc\"\n",
    "    else:                   return pos_map[data]\n",
    "\n",
    "words_pos['POS2'] = words_pos['POS'].apply(get_general_POS)\n",
    "words_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put all of this data into a dictionary, so we can simply look up the part of speech of any word we encounter in the test set. It is important to note that the part of speech of some of these words will change based on the context that it is used in. Ideally, we would not have any of these ambiguities in the data, but for now, we can assume this is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stores all of the parts of speech in a dictionary\n",
    "phrase_iter = words_pos.Phrase.values\n",
    "pos_iter = words_pos.POS2.values\n",
    "part_of_speech = dict(zip(phrase_iter, pos_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, we noticed that the phrases in the data had a wide range of lengths. In order to get a feeling of how the words were classified, we only looked at the sentiments of phrases containing single words first. From the data, we built a dictionary that stores all of the one-word phrases and their associated sentiments.\n",
    "\n",
    "We also tried building a corpus of all two-word phrases, but that ended up overfitting the training data pretty badly. The data has $N$ unique words, so theoretically, there are around $N^2$ unique word pairs. The training data only has a small portion of all the $N^2$ combinations, causing our model to overfit to a small set of quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracts one-word phrases using a regular expression\n",
    "single_words = train[~train.Phrase.str.contains(' ')]\n",
    "single_words = single_words[single_words.Phrase.str.contains('^[a-zA-Z]+$')]\n",
    "single_words.Phrase = single_words.Phrase.str.lower()\n",
    "\n",
    "# Creates a dictionary mapping words -> sentiment\n",
    "phrase_iter = single_words.Phrase.values\n",
    "sent_iter = single_words.Sentiment.values\n",
    "corpus = dict(zip(phrase_iter, sent_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also looked at negations and how they might affect the model that we will implement later. Although \"a particularly good film\" has a very positive sentiment (4), \"It's not a particularly good film\" causes the sentiment to drop to being negative (1). When we are designing our model, this is definitely something we should look out for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587    4\n",
      "Name: Sentiment, dtype: int64\n",
      "2584    1\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print train[train.Phrase == \"a particularly good film\"].Sentiment\n",
    "print train[train.Phrase == \"It 's not a particularly good film\"].Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correctly flip the sentiment, we located all the words after a negation and prepended them with the string “NOT\\_”. For instance, the string \"not a particularly good film\" will transform into \"not NOT_a NOT_particularly NOT_good NOT_film.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepend_NOT(match):\n",
    "    \"\"\"\n",
    "    A function that feeds into a regular expression substitution function\n",
    "    that prepends all words after a negation word (i.e. \"didn't\" and\n",
    "    \"not\") with \"NOT_\".\n",
    "    \"\"\"\n",
    "    match = match.group()\n",
    "    words = match.split(\" \")\n",
    "    negation = words[0]\n",
    "    del words[0]\n",
    "    new_words = [\"NOT_\" + word for word in words]\n",
    "    return negation + \" \" + \" \".join(new_words)\n",
    "\n",
    "\n",
    "def substitute_negations(phrase):\n",
    "    \"\"\"\n",
    "    Replaces input phrase with the same phrase, except prepending a \"NOT_\"\n",
    "    for every word after a negation word (i.e. \"didn't\" and \"not\"). This\n",
    "    can only occur in phrases with more than one word.\n",
    "    \"\"\"\n",
    "    # negation_words is a list of regular expressions\n",
    "    negation_words = [r\"not\", r\"n't\", r\"no\"]\n",
    "    \n",
    "    # negation_words then gets turned into a regular expression string\n",
    "    negation_words = [r\"(\" + word + r\")\" for word in negation_words]\n",
    "    negation_words = (r\"|\").join(negation_words)\n",
    "    \n",
    "    negations_re = re.compile(r\"(\" + negation_words + r\")[A-z ']*\")\n",
    "    substitution = negations_re.sub(prepend_NOT, phrase)\n",
    "    \n",
    "    if substitution == \"\":\n",
    "        return phrase\n",
    "    return substitution\n",
    "\n",
    "\n",
    "def add_NOT_to_negations(df):\n",
    "    \"\"\"\n",
    "    Replaces each phrase in the dataframe with the same phrase, but\n",
    "    replacing every word after a negation word (i.e. \"didn't\" and \"not\")\n",
    "    with \"NOT_\" prepended to the word. This can only occur in phrases\n",
    "    with more than one word.\n",
    "    \"\"\"\n",
    "    data = df\n",
    "    data[\"Negations\"] = data[\"Phrase\"].apply(lambda x: substitute_negations(x))\n",
    "    return data\n",
    "\n",
    "train = add_NOT_to_negations(train)\n",
    "test = add_NOT_to_negations(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each word in the corpus, we need to add a new word in a new dictionary that has all of the \"NOT\\_\" words. The sentiment also has to be flipped, so that 0 maps to 4, and 1 maps to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not good = negative sentiment: 1\n",
      "not bad = positive sentiment: 4\n"
     ]
    }
   ],
   "source": [
    "# Generates a corpus of all the negated words\n",
    "not_corpus = {}\n",
    "for w in corpus:\n",
    "    words = w.split()\n",
    "    if len(words) == 1:\n",
    "        not_corpus[\"NOT_\" + w] = abs(4 - corpus[w])\n",
    "    elif len(words) == 2:\n",
    "        not_corpus[\"NOT_\" + words[0] + \" NOT_\" + words[1]] = abs(4 - corpus[w])\n",
    "    \n",
    "print \"not good = negative sentiment: \" + str(not_corpus[\"NOT_good\"])\n",
    "print \"not bad = positive sentiment: \" + str(not_corpus[\"NOT_bad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a couple of graphs, we gleaned that most of nearly all the sentences consisted of neutral words. The lengths of each word are also Gaussian distributions with about the same mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f5e22beca50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA38AAAGLCAYAAAB3KyupAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSVAGGP0mSJEmqgP8PcZzOhFDYBPoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e258b1350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots a histogram of the word sentiment, by word length\n",
    "plt.figure(figsize=(15, 6))\n",
    "ax = sns.countplot(x=\"Sentiment\", data=single_words, hue=single_words.Phrase.str.len(), palette=\"Greens_d\")\n",
    "ax.legend().set_visible(False)\n",
    "plt.title(\"Histogram of Word Sentiment (Hue = Word Length)\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that one key positive or negative term completely changes the sentiment of the sentence. As a simple model, we used this observation to calculate the sentiment of each phrase by using the sentiment of the most extreme word in each phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Splits the phrase into a list of words '''\n",
    "def get_words(phrase):\n",
    "    words = phrase.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(w.lower().strip('.,;!?\"()'))\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "''' Gets the most extreme sentiment value in the sentence and returns it '''\n",
    "def get_extreme_sentiment(phrase):\n",
    "    words = get_words(phrase)\n",
    "    most_sentiment = 2\n",
    "    \n",
    "    for w in words:\n",
    "        if w in corpus:\n",
    "            sentiment = corpus[w]\n",
    "            if abs(sentiment - 2) >= abs(most_sentiment - 2):\n",
    "                most_sentiment = sentiment\n",
    "        if w in not_corpus:\n",
    "            sentiment = not_corpus[w]\n",
    "            if abs(sentiment - 2) >= abs(most_sentiment - 2):\n",
    "                most_sentiment = sentiment\n",
    "        \n",
    "    return most_sentiment\n",
    "\n",
    "# Predicts the first 5 sentiment values\n",
    "predictions = test.Phrase.apply(get_extreme_sentiment)\n",
    "\n",
    "# Generates the csv file\n",
    "submission = pd.DataFrame({\n",
    "        \"PhraseId\": test[\"PhraseId\"],\n",
    "        \"Sentiment\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"benchmark_iteration.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model ended up being 54.682% accurate on Kaggle, which isn't terrible. However, we can improve the model slightly by using a Naive Bayes approach instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes model estimates the probability that a document belongs to a certain sentiment class. The simplified formula for the model is shown below:\n",
    "\n",
    "$c = \\max_\\limits{c\\:\\in\\:C}\\:\\Big( P(c)\\: \\cdot \\prod_\\limits{x\\:\\in\\:X} P(x\\:|\\:c)\\Big)$\n",
    "\n",
    "We need to calculate $P(c)$, the probability of a sentiment class appearing in the data, and for each word, P(x\\:|\\:c), the probability that a word $x$ appears in sentiment class $c$. We started with $P(c)$, since it was easier. The histogram below shows the distribution of all the classes in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEbCAYAAADajfNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAyxrUKYeFQ90AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e257e76d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots the sentiment histogram on a semilog-y plot\n",
    "sns.countplot(x=\"Sentiment\", data=train, palette=\"Greens_d\")\n",
    "plt.title(\"Histogram of Phrase Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty obvious from this that we're most likely to encounter a phrase that is neutral. So, $P(c)$ for sentiment 2 should be the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr{Sentiment 0}: 0.0453\n",
      "Pr{Sentiment 1}: 0.1747\n",
      "Pr{Sentiment 2}: 0.5099\n",
      "Pr{Sentiment 3}: 0.2109\n",
      "Pr{Sentiment 4}: 0.0589\n"
     ]
    }
   ],
   "source": [
    "# Calculates the probability of any given phrase having a particular sentiment\n",
    "P_c = []\n",
    "for i in range(5):\n",
    "    prob = len(train[train.Sentiment == i]) / float(len(train))\n",
    "    P_c.append(prob)\n",
    "    \n",
    "print \"Pr{Sentiment 0}: \" + str(P_c[0])[:6]\n",
    "print \"Pr{Sentiment 1}: \" + str(P_c[1])[:6]\n",
    "print \"Pr{Sentiment 2}: \" + str(P_c[2])[:6]\n",
    "print \"Pr{Sentiment 3}: \" + str(P_c[3])[:6]\n",
    "print \"Pr{Sentiment 4}: \" + str(P_c[4])[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to figure out how probable it is for a word to appear in a certain sentiment class. We started by counting how many times each word appears in all the phrases of a sentiment class. It is very likely, for instance, for the word \"exciting\" to appear in very positive phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many times does the word \"exciting\" appear in sentiment 4 phrases?\n",
      "8\n",
      "How many times does the word \"exciting\" appear in sentiment 0 phrases?\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Extracts just the sentences from the dataset and combines them into mega-strings\n",
    "sentences_dict = train['SentenceId'].drop_duplicates()\n",
    "sentences = train.iloc[sentences_dict.keys()]\n",
    "sentences.head()\n",
    "\n",
    "# For each sentiment's mega-string, counts the number of times each word appears\n",
    "text = []\n",
    "for i in range(5):\n",
    "    d = {}\n",
    "    temp = sentences[sentences.Sentiment == i]\n",
    "    words_in_class = ' '.join(temp.Phrase.values).lower().split()\n",
    "    for w in words_in_class:\n",
    "        new_word = w.lower().strip('.,;!?\"()')\n",
    "        if not new_word.strip(string.ascii_letters) and new_word != '':\n",
    "            d[new_word] = d.get(new_word,0) + 1\n",
    "\n",
    "    text.append(d)\n",
    "\n",
    "print \"How many times does the word \\\"exciting\\\" appear in sentiment 4 phrases?\"\n",
    "print text[4]['exciting']\n",
    "print \"How many times does the word \\\"exciting\\\" appear in sentiment 0 phrases?\"\n",
    "print text[0]['exciting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the probability of the word appearing in a sentiment class $P(w\\: | \\: c)$ needs to be calculated. This can be done by simply dividing each of the counts by the total number of words in the dictionary. However, we need to multiply the probabilities together, so if a word appears 0 times, the entire product will go to zero. In order to avoid this, we added Laplace (or plus-one) smoothing, so that the probability will never be zero.\n",
    "\n",
    "The expression $\\frac{0}{N}$ becomes $\\frac{1}{N + len(V)}$ when the smoothing is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the word \"yellow\" appearing in a sentiment 2 phrase: \n",
      "5.01957634776e-05\n"
     ]
    }
   ],
   "source": [
    "# Calculates the probability of a word being in a particular class for every word in the corpus\n",
    "P_wc = []\n",
    "alpha = 1.0 # Laplace smoothing\n",
    "for d in text:\n",
    "    n_words_in_class = sum(d.values())\n",
    "    new_dict = {}\n",
    "    for w in corpus:\n",
    "        if w in d:\n",
    "            new_dict[w] = (d[w] + alpha) / (n_words_in_class + alpha*len(corpus))\n",
    "        else:\n",
    "            new_dict[w] = alpha / len(corpus)\n",
    "    for w in not_corpus:\n",
    "        if w in d:\n",
    "            new_dict[w] = (d[w] + alpha) / (n_words_in_class + alpha*len(corpus))\n",
    "        else:\n",
    "            new_dict[w] = alpha / len(corpus)\n",
    "    P_wc.append(new_dict)\n",
    "    \n",
    "print \"Probability of the word \\\"yellow\\\" appearing in a sentiment 2 phrase: \"\n",
    "print P_wc[2]['yellow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that needs to be done is to multiply all of the probabilities in the way shown in the equation earlier. This is done for each sentiment class, and the sentiment with the highest probability will become the sentiment of the phrase.\n",
    "\n",
    "We made a couple improvements to the base Naive Bayes model. First, we ignored all words that were labeled \"miscellaneous\" by the part-of-speech tagger. We also used Pattern's sentiment analysis tool to help calculate the sentiment of phrases that our model perceived to be neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Splits the phrase into a list of words '''\n",
    "def get_words(phrase):\n",
    "    words = phrase.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(w.lower().strip('.,;!?\"()'))\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "''' Calculates the sentiment of a phrase using a Naive bayes classifier '''\n",
    "def naive_bayes(phrase):\n",
    "    if phrase in corpus:\n",
    "        return corpus[phrase]\n",
    "    if phrase in not_corpus:\n",
    "        return not_corpus[phrase]\n",
    "    \n",
    "    P_cw = 0 # Probability that the phrase has a sentiment c\n",
    "    words = get_words(phrase)\n",
    "    \n",
    "    for s in range(5):\n",
    "        # Probability of the sentiment occurring\n",
    "        prob = P_c[s]\n",
    "        \n",
    "        # Probability of the word appearing in the sentiment class\n",
    "        for w in words:\n",
    "            # Ignores words with miscellaneous parts of speech\n",
    "            if w in corpus and part_of_speech[w] != \"misc\":\n",
    "                prob *= P_wc[s][w]\n",
    "            if w in not_corpus and part_of_speech[w[4:]] != \"misc\":\n",
    "                prob *= P_wc[s][w]\n",
    "            \n",
    "        if prob > P_cw:\n",
    "            P_cw = prob\n",
    "            sentiment = s\n",
    "            \n",
    "    # Double checks the sentiment using Pattern's sentiment analysis package\n",
    "    if sentiment == 2:\n",
    "        pattern_sent = pattern.en.sentiment(phrase)\n",
    "        if abs(pattern_sent[0]) > 0.1:\n",
    "            sentiment += int(round(pattern_sent[0] * 2))\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy on the training data improved! (It was .547 before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.559284890427\n"
     ]
    }
   ],
   "source": [
    "train[\"Predictions\"] = train.Phrase.apply(naive_bayes)\n",
    "print \"Train accuracy: \" + str(len(train[train.Sentiment == train.Predictions]) / float(len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score we got on Kaggle for this submission was 56.987%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates the csv file\n",
    "predictions = test.Phrase.apply(naive_bayes)\n",
    "submission = pd.DataFrame({\n",
    "        \"PhraseId\": test[\"PhraseId\"],\n",
    "        \"Sentiment\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"final_iteration.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few glaring drawbacks to the Naive Bayes approach, mainly due to the fact that the bag-of-words model assumes that the ordering of the words does not matter. Sentences that have more complex structures can be weighted differently based on what words come first. Additionally, the model does not do a very good job when it sees a bunch of adjectives in a row, such as \"intrigue, betrayal, decit, and murder\" or \"quite, introspective, and entertaining.\" In both examples, the authors probably wanted to create emphasis, and the model should be able to detect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>POS</th>\n",
       "      <th>Negations</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, NN, IN, NNS, VBG, DT, NN, IN, WP, VBZ, JJ...</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[DT, NN, IN, NNS, VBG, DT, NN, IN, WP, VBZ, JJ...</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "      <td>[IN, NNS, VBG, DT, NN, IN, WP, VBZ, JJ, IN, DT...</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "      <td>[NNS, VBG, DT, NN, IN, WP, VBZ, JJ, IN, DT, NN]</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBG, DT, NN, IN, WP, VBZ, JJ, IN, DT, NN]</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>[DT, WP, VBZ, JJ, IN, DT, NN]</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>[WP, VBZ, JJ, IN, DT, NN]</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBZ, JJ, IN, DT, NN]</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBZ, RB, JJ, IN, DT, NN, ,, DT, IN, WDT, RB, ...</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBZ, RB, JJ, IN, DT, NN, ,, DT, IN, WDT, RB, ...</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>good for the gander , some of which occasional...</td>\n",
       "      <td>2</td>\n",
       "      <td>[JJ, IN, DT, NN, ,, DT, IN, WDT, RB, VBZ, CC, ...</td>\n",
       "      <td>good for the gander , some of which occasional...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander , some of which occasionally amuses...</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, NN, ,, DT, IN, WDT, RB, VBZ, CC, NN, IN, ...</td>\n",
       "      <td>the gander , some of which occasionally amuses...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "      <td>[DT, JJ, ,, JJ, CC, JJ, JJ, VBZ, JJ, VBG, .]</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>3</td>\n",
       "      <td>[DT, JJ, ,, JJ, CC, JJ, JJ]</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining indepen...</td>\n",
       "      <td>4</td>\n",
       "      <td>[JJ, ,, JJ, CC, JJ, JJ]</td>\n",
       "      <td>quiet , introspective and entertaining indepen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "      <td>[NN, CC]</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "      <td>[VBZ, JJ, VBG]</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBZ, JJ]</td>\n",
       "      <td>is worth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work</td>\n",
       "      <td>2</td>\n",
       "      <td>[RB, NNS, IN, NNP, NNP, POS, NN]</td>\n",
       "      <td>Even fans of Ismail Merchant 's work</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>[MD, VB, DT, JJ, NN, VBG, IN, DT, NN]</td>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>[VB, DT, JJ, NN, VBG, IN, DT, NN]</td>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>sitting through this one</td>\n",
       "      <td>1</td>\n",
       "      <td>[VBG, IN, DT, NN]</td>\n",
       "      <td>sitting through this one</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>4</td>\n",
       "      <td>[DT, RB, VBG, NN, IN, NN, CC, PDT, DT, NN, ,, ...</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>4</td>\n",
       "      <td>[DT, RB, VBG, NN, IN, NN, CC, PDT, DT, NN, ,, ...</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>the intrigue , betrayal , deceit and murder</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, NN, ,, NN, ,, NN, CC, NN]</td>\n",
       "      <td>the intrigue , betrayal , deceit and murder</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>136</td>\n",
       "      <td>4</td>\n",
       "      <td>betrayal , deceit and murder</td>\n",
       "      <td>1</td>\n",
       "      <td>[NN, ,, NN, CC, NN]</td>\n",
       "      <td>betrayal , deceit and murder</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>, deceit and murder</td>\n",
       "      <td>1</td>\n",
       "      <td>[,, NN, CC, NN]</td>\n",
       "      <td>, deceit and murder</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>139</td>\n",
       "      <td>4</td>\n",
       "      <td>deceit and murder</td>\n",
       "      <td>1</td>\n",
       "      <td>[NN, CC, NN]</td>\n",
       "      <td>deceit and murder</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>4</td>\n",
       "      <td>a Shakespearean tragedy</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, JJ, NN]</td>\n",
       "      <td>a Shakespearean tragedy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>a juicy soap opera</td>\n",
       "      <td>3</td>\n",
       "      <td>[DT, NN, NN, NN]</td>\n",
       "      <td>a juicy soap opera</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155992</th>\n",
       "      <td>155993</td>\n",
       "      <td>8540</td>\n",
       "      <td>to go with this claustrophobic concept</td>\n",
       "      <td>1</td>\n",
       "      <td>[TO, VB, IN, DT, NN, NN]</td>\n",
       "      <td>to go with this claustrophobic concept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155993</th>\n",
       "      <td>155994</td>\n",
       "      <td>8540</td>\n",
       "      <td>go with this claustrophobic concept</td>\n",
       "      <td>1</td>\n",
       "      <td>[VB, IN, DT, NN, NN]</td>\n",
       "      <td>go with this claustrophobic concept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155994</th>\n",
       "      <td>155995</td>\n",
       "      <td>8540</td>\n",
       "      <td>with this claustrophobic concept</td>\n",
       "      <td>2</td>\n",
       "      <td>[IN, DT, NN, NN]</td>\n",
       "      <td>with this claustrophobic concept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155995</th>\n",
       "      <td>155996</td>\n",
       "      <td>8540</td>\n",
       "      <td>this claustrophobic concept</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, NN, NN]</td>\n",
       "      <td>this claustrophobic concept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155996</th>\n",
       "      <td>155997</td>\n",
       "      <td>8540</td>\n",
       "      <td>claustrophobic concept</td>\n",
       "      <td>1</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>claustrophobic concept</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155997</th>\n",
       "      <td>155998</td>\n",
       "      <td>8541</td>\n",
       "      <td>Despite these annoyances , the capable Claybur...</td>\n",
       "      <td>2</td>\n",
       "      <td>[IN, DT, NNS, ,, DT, JJ, NNP, CC, NNP, RB, VBP...</td>\n",
       "      <td>Despite these annoyances NOT_, the capable Cla...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155999</th>\n",
       "      <td>156000</td>\n",
       "      <td>8541</td>\n",
       "      <td>these annoyances</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, NNS]</td>\n",
       "      <td>these annoyances</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156005</th>\n",
       "      <td>156006</td>\n",
       "      <td>8541</td>\n",
       "      <td>the capable Clayburgh</td>\n",
       "      <td>3</td>\n",
       "      <td>[DT, JJ, NNP]</td>\n",
       "      <td>the capable Clayburgh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156006</th>\n",
       "      <td>156007</td>\n",
       "      <td>8541</td>\n",
       "      <td>capable Clayburgh</td>\n",
       "      <td>1</td>\n",
       "      <td>[JJ, NNP]</td>\n",
       "      <td>capable Clayburgh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156007</th>\n",
       "      <td>156008</td>\n",
       "      <td>8541</td>\n",
       "      <td>really do a great job of anchoring the charact...</td>\n",
       "      <td>4</td>\n",
       "      <td>[RB, VB, DT, JJ, NN, IN, VBG, DT, NNS, IN, DT,...</td>\n",
       "      <td>really do a great job of anchoring the charact...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156010</th>\n",
       "      <td>156011</td>\n",
       "      <td>8541</td>\n",
       "      <td>a great job of anchoring the characters in the...</td>\n",
       "      <td>4</td>\n",
       "      <td>[DT, JJ, NN, IN, VBG, DT, NNS, IN, DT, JJ, NNS...</td>\n",
       "      <td>a great job of anchoring the characters in the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156014</th>\n",
       "      <td>156015</td>\n",
       "      <td>8541</td>\n",
       "      <td>anchoring the characters in the emotional real...</td>\n",
       "      <td>3</td>\n",
       "      <td>[VBG, DT, NNS, IN, DT, JJ, NNS, IN, JJ, NN]</td>\n",
       "      <td>anchoring the characters in the emotional real...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156023</th>\n",
       "      <td>156024</td>\n",
       "      <td>8542</td>\n",
       "      <td>-LRB- Tries -RRB-</td>\n",
       "      <td>3</td>\n",
       "      <td>[JJ, NNS, VBP]</td>\n",
       "      <td>-LRB- Tries -RRB-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156028</th>\n",
       "      <td>156029</td>\n",
       "      <td>8542</td>\n",
       "      <td>'s already a joke in the United States .</td>\n",
       "      <td>1</td>\n",
       "      <td>[POS, RB, DT, NN, IN, DT, NNP, NNPS, .]</td>\n",
       "      <td>'s already a joke in the United States .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156029</th>\n",
       "      <td>156030</td>\n",
       "      <td>8542</td>\n",
       "      <td>'s already a joke in the United States</td>\n",
       "      <td>1</td>\n",
       "      <td>[POS, RB, DT, NN, IN, DT, NNP, NNPS]</td>\n",
       "      <td>'s already a joke in the United States</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156032</th>\n",
       "      <td>156033</td>\n",
       "      <td>8543</td>\n",
       "      <td>The movie 's downfall</td>\n",
       "      <td>1</td>\n",
       "      <td>[DT, NN, POS, NN]</td>\n",
       "      <td>The movie 's downfall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156035</th>\n",
       "      <td>156036</td>\n",
       "      <td>8543</td>\n",
       "      <td>to substitute plot for personality</td>\n",
       "      <td>2</td>\n",
       "      <td>[TO, VB, NN, IN, NN]</td>\n",
       "      <td>to substitute plot for personality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156039</th>\n",
       "      <td>156040</td>\n",
       "      <td>8544</td>\n",
       "      <td>The film is darkly atmospheric , with Herrmann...</td>\n",
       "      <td>2</td>\n",
       "      <td>[DT, NN, VBZ, JJ, JJ, ,, IN, NNP, RB, VBG, DT,...</td>\n",
       "      <td>The film is darkly atmospheric , with Herrmann...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156040</th>\n",
       "      <td>156041</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBZ, JJ, JJ, ,, IN, NNP, RB, VBG, DT, NN, CC,...</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156041</th>\n",
       "      <td>156042</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBZ, JJ, JJ, ,, IN, NNP, RB, VBG, DT, NN, CC,...</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156043</th>\n",
       "      <td>156044</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric</td>\n",
       "      <td>3</td>\n",
       "      <td>[VBZ, JJ, NN]</td>\n",
       "      <td>is darkly atmospheric</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156044</th>\n",
       "      <td>156045</td>\n",
       "      <td>8544</td>\n",
       "      <td>with Herrmann quietly suggesting the sadness a...</td>\n",
       "      <td>2</td>\n",
       "      <td>[IN, NNP, RB, VBG, DT, NN, CC, NN, NN, NNP, PO...</td>\n",
       "      <td>with Herrmann quietly suggesting the sadness a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156045</th>\n",
       "      <td>156046</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann quietly suggesting the sadness and ob...</td>\n",
       "      <td>2</td>\n",
       "      <td>[NNP, RB, VBG, DT, NN, CC, NN, NN, NNP, POS, J...</td>\n",
       "      <td>Herrmann quietly suggesting the sadness and ob...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156047</th>\n",
       "      <td>156048</td>\n",
       "      <td>8544</td>\n",
       "      <td>quietly suggesting the sadness and obsession b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[RB, VBG, DT, NN, CC, NN, NN, NNP, POS, JJ, JJ...</td>\n",
       "      <td>quietly suggesting the sadness and obsession b...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156048</th>\n",
       "      <td>156049</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession beneath H...</td>\n",
       "      <td>2</td>\n",
       "      <td>[VBG, DT, NN, CC, NN, NN, NNP, POS, JJ, JJ, NNS]</td>\n",
       "      <td>suggesting the sadness and obsession beneath H...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156051</th>\n",
       "      <td>156052</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and obsession</td>\n",
       "      <td>1</td>\n",
       "      <td>[NN, CC, NN]</td>\n",
       "      <td>sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156052</th>\n",
       "      <td>156053</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and</td>\n",
       "      <td>1</td>\n",
       "      <td>[NN, CC]</td>\n",
       "      <td>sadness and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156053</th>\n",
       "      <td>156054</td>\n",
       "      <td>8544</td>\n",
       "      <td>beneath Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>[NN, NNP, POS, JJ, JJ, NNS]</td>\n",
       "      <td>beneath Hearst 's forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156054</th>\n",
       "      <td>156055</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>[NNP, POS, JJ, JJ, NNS]</td>\n",
       "      <td>Hearst 's forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68778 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "1              2           1   \n",
       "5              6           1   \n",
       "7              8           1   \n",
       "9             10           1   \n",
       "15            16           1   \n",
       "17            18           1   \n",
       "19            20           1   \n",
       "27            28           1   \n",
       "28            29           1   \n",
       "31            32           1   \n",
       "33            34           1   \n",
       "63            64           2   \n",
       "64            65           2   \n",
       "66            67           2   \n",
       "71            72           2   \n",
       "77            78           2   \n",
       "78            79           2   \n",
       "82            83           3   \n",
       "101          102           3   \n",
       "103          104           3   \n",
       "110          111           3   \n",
       "117          118           4   \n",
       "118          119           4   \n",
       "131          132           4   \n",
       "135          136           4   \n",
       "137          138           4   \n",
       "138          139           4   \n",
       "145          146           4   \n",
       "150          151           4   \n",
       "...          ...         ...   \n",
       "155992    155993        8540   \n",
       "155993    155994        8540   \n",
       "155994    155995        8540   \n",
       "155995    155996        8540   \n",
       "155996    155997        8540   \n",
       "155997    155998        8541   \n",
       "155999    156000        8541   \n",
       "156005    156006        8541   \n",
       "156006    156007        8541   \n",
       "156007    156008        8541   \n",
       "156010    156011        8541   \n",
       "156014    156015        8541   \n",
       "156023    156024        8542   \n",
       "156028    156029        8542   \n",
       "156029    156030        8542   \n",
       "156032    156033        8543   \n",
       "156035    156036        8543   \n",
       "156039    156040        8544   \n",
       "156040    156041        8544   \n",
       "156041    156042        8544   \n",
       "156043    156044        8544   \n",
       "156044    156045        8544   \n",
       "156045    156046        8544   \n",
       "156047    156048        8544   \n",
       "156048    156049        8544   \n",
       "156051    156052        8544   \n",
       "156052    156053        8544   \n",
       "156053    156054        8544   \n",
       "156054    156055        8544   \n",
       "156057    156058        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "0       A series of escapades demonstrating the adage ...          1   \n",
       "1       A series of escapades demonstrating the adage ...          2   \n",
       "5       of escapades demonstrating the adage that what...          2   \n",
       "7       escapades demonstrating the adage that what is...          2   \n",
       "9       demonstrating the adage that what is good for ...          2   \n",
       "15                        that what is good for the goose          2   \n",
       "17                             what is good for the goose          2   \n",
       "19                                  is good for the goose          2   \n",
       "27      is also good for the gander , some of which oc...          2   \n",
       "28      is also good for the gander , some of which oc...          2   \n",
       "31      good for the gander , some of which occasional...          2   \n",
       "33      the gander , some of which occasionally amuses...          1   \n",
       "63      This quiet , introspective and entertaining in...          4   \n",
       "64      This quiet , introspective and entertaining in...          3   \n",
       "66      quiet , introspective and entertaining indepen...          4   \n",
       "71                                      introspective and          3   \n",
       "77                                       is worth seeking          4   \n",
       "78                                               is worth          2   \n",
       "82                   Even fans of Ismail Merchant 's work          2   \n",
       "101       would have a hard time sitting through this one          0   \n",
       "103             have a hard time sitting through this one          0   \n",
       "110                              sitting through this one          1   \n",
       "117     A positively thrilling combination of ethnogra...          4   \n",
       "118     A positively thrilling combination of ethnogra...          4   \n",
       "131           the intrigue , betrayal , deceit and murder          1   \n",
       "135                          betrayal , deceit and murder          1   \n",
       "137                                   , deceit and murder          1   \n",
       "138                                     deceit and murder          1   \n",
       "145                               a Shakespearean tragedy          1   \n",
       "150                                    a juicy soap opera          3   \n",
       "...                                                   ...        ...   \n",
       "155992             to go with this claustrophobic concept          1   \n",
       "155993                go with this claustrophobic concept          1   \n",
       "155994                   with this claustrophobic concept          2   \n",
       "155995                        this claustrophobic concept          1   \n",
       "155996                             claustrophobic concept          1   \n",
       "155997  Despite these annoyances , the capable Claybur...          2   \n",
       "155999                                   these annoyances          1   \n",
       "156005                              the capable Clayburgh          3   \n",
       "156006                                  capable Clayburgh          1   \n",
       "156007  really do a great job of anchoring the charact...          4   \n",
       "156010  a great job of anchoring the characters in the...          4   \n",
       "156014  anchoring the characters in the emotional real...          3   \n",
       "156023                                  -LRB- Tries -RRB-          3   \n",
       "156028           's already a joke in the United States .          1   \n",
       "156029             's already a joke in the United States          1   \n",
       "156032                              The movie 's downfall          1   \n",
       "156035                 to substitute plot for personality          2   \n",
       "156039  The film is darkly atmospheric , with Herrmann...          2   \n",
       "156040  is darkly atmospheric , with Herrmann quietly ...          2   \n",
       "156041  is darkly atmospheric , with Herrmann quietly ...          2   \n",
       "156043                              is darkly atmospheric          3   \n",
       "156044  with Herrmann quietly suggesting the sadness a...          2   \n",
       "156045  Herrmann quietly suggesting the sadness and ob...          2   \n",
       "156047  quietly suggesting the sadness and obsession b...          1   \n",
       "156048  suggesting the sadness and obsession beneath H...          2   \n",
       "156051                              sadness and obsession          1   \n",
       "156052                                        sadness and          1   \n",
       "156053        beneath Hearst 's forced avuncular chortles          2   \n",
       "156054                Hearst 's forced avuncular chortles          2   \n",
       "156057                                 avuncular chortles          3   \n",
       "\n",
       "                                                      POS  \\\n",
       "0       [DT, NN, IN, NNS, VBG, DT, NN, IN, WP, VBZ, JJ...   \n",
       "1       [DT, NN, IN, NNS, VBG, DT, NN, IN, WP, VBZ, JJ...   \n",
       "5       [IN, NNS, VBG, DT, NN, IN, WP, VBZ, JJ, IN, DT...   \n",
       "7         [NNS, VBG, DT, NN, IN, WP, VBZ, JJ, IN, DT, NN]   \n",
       "9              [VBG, DT, NN, IN, WP, VBZ, JJ, IN, DT, NN]   \n",
       "15                          [DT, WP, VBZ, JJ, IN, DT, NN]   \n",
       "17                              [WP, VBZ, JJ, IN, DT, NN]   \n",
       "19                                  [VBZ, JJ, IN, DT, NN]   \n",
       "27      [VBZ, RB, JJ, IN, DT, NN, ,, DT, IN, WDT, RB, ...   \n",
       "28      [VBZ, RB, JJ, IN, DT, NN, ,, DT, IN, WDT, RB, ...   \n",
       "31      [JJ, IN, DT, NN, ,, DT, IN, WDT, RB, VBZ, CC, ...   \n",
       "33      [DT, NN, ,, DT, IN, WDT, RB, VBZ, CC, NN, IN, ...   \n",
       "63           [DT, JJ, ,, JJ, CC, JJ, JJ, VBZ, JJ, VBG, .]   \n",
       "64                            [DT, JJ, ,, JJ, CC, JJ, JJ]   \n",
       "66                                [JJ, ,, JJ, CC, JJ, JJ]   \n",
       "71                                               [NN, CC]   \n",
       "77                                         [VBZ, JJ, VBG]   \n",
       "78                                              [VBZ, JJ]   \n",
       "82                       [RB, NNS, IN, NNP, NNP, POS, NN]   \n",
       "101                 [MD, VB, DT, JJ, NN, VBG, IN, DT, NN]   \n",
       "103                     [VB, DT, JJ, NN, VBG, IN, DT, NN]   \n",
       "110                                     [VBG, IN, DT, NN]   \n",
       "117     [DT, RB, VBG, NN, IN, NN, CC, PDT, DT, NN, ,, ...   \n",
       "118     [DT, RB, VBG, NN, IN, NN, CC, PDT, DT, NN, ,, ...   \n",
       "131                        [DT, NN, ,, NN, ,, NN, CC, NN]   \n",
       "135                                   [NN, ,, NN, CC, NN]   \n",
       "137                                       [,, NN, CC, NN]   \n",
       "138                                          [NN, CC, NN]   \n",
       "145                                          [DT, JJ, NN]   \n",
       "150                                      [DT, NN, NN, NN]   \n",
       "...                                                   ...   \n",
       "155992                           [TO, VB, IN, DT, NN, NN]   \n",
       "155993                               [VB, IN, DT, NN, NN]   \n",
       "155994                                   [IN, DT, NN, NN]   \n",
       "155995                                       [DT, NN, NN]   \n",
       "155996                                           [NN, NN]   \n",
       "155997  [IN, DT, NNS, ,, DT, JJ, NNP, CC, NNP, RB, VBP...   \n",
       "155999                                          [DT, NNS]   \n",
       "156005                                      [DT, JJ, NNP]   \n",
       "156006                                          [JJ, NNP]   \n",
       "156007  [RB, VB, DT, JJ, NN, IN, VBG, DT, NNS, IN, DT,...   \n",
       "156010  [DT, JJ, NN, IN, VBG, DT, NNS, IN, DT, JJ, NNS...   \n",
       "156014        [VBG, DT, NNS, IN, DT, JJ, NNS, IN, JJ, NN]   \n",
       "156023                                     [JJ, NNS, VBP]   \n",
       "156028            [POS, RB, DT, NN, IN, DT, NNP, NNPS, .]   \n",
       "156029               [POS, RB, DT, NN, IN, DT, NNP, NNPS]   \n",
       "156032                                  [DT, NN, POS, NN]   \n",
       "156035                               [TO, VB, NN, IN, NN]   \n",
       "156039  [DT, NN, VBZ, JJ, JJ, ,, IN, NNP, RB, VBG, DT,...   \n",
       "156040  [VBZ, JJ, JJ, ,, IN, NNP, RB, VBG, DT, NN, CC,...   \n",
       "156041  [VBZ, JJ, JJ, ,, IN, NNP, RB, VBG, DT, NN, CC,...   \n",
       "156043                                      [VBZ, JJ, NN]   \n",
       "156044  [IN, NNP, RB, VBG, DT, NN, CC, NN, NN, NNP, PO...   \n",
       "156045  [NNP, RB, VBG, DT, NN, CC, NN, NN, NNP, POS, J...   \n",
       "156047  [RB, VBG, DT, NN, CC, NN, NN, NNP, POS, JJ, JJ...   \n",
       "156048   [VBG, DT, NN, CC, NN, NN, NNP, POS, JJ, JJ, NNS]   \n",
       "156051                                       [NN, CC, NN]   \n",
       "156052                                           [NN, CC]   \n",
       "156053                        [NN, NNP, POS, JJ, JJ, NNS]   \n",
       "156054                            [NNP, POS, JJ, JJ, NNS]   \n",
       "156057                                          [JJ, NNS]   \n",
       "\n",
       "                                                Negations  Predictions  \n",
       "0       A series of escapades demonstrating the adage ...            3  \n",
       "1       A series of escapades demonstrating the adage ...            3  \n",
       "5       of escapades demonstrating the adage that what...            3  \n",
       "7       escapades demonstrating the adage that what is...            3  \n",
       "9       demonstrating the adage that what is good for ...            3  \n",
       "15                        that what is good for the goose            3  \n",
       "17                             what is good for the goose            3  \n",
       "19                                  is good for the goose            3  \n",
       "27      is also good for the gander , some of which oc...            3  \n",
       "28      is also good for the gander , some of which oc...            3  \n",
       "31      good for the gander , some of which occasional...            3  \n",
       "33      the gander , some of which occasionally amuses...            2  \n",
       "63      This quiet , introspective and entertaining in...            2  \n",
       "64      This quiet , introspective and entertaining in...            2  \n",
       "66      quiet , introspective and entertaining indepen...            2  \n",
       "71                                      introspective and            2  \n",
       "77                                       is worth seeking            3  \n",
       "78                                               is worth            3  \n",
       "82                   Even fans of Ismail Merchant 's work            3  \n",
       "101       would have a hard time sitting through this one            1  \n",
       "103             have a hard time sitting through this one            1  \n",
       "110                              sitting through this one            2  \n",
       "117     A positively thrilling combination of ethnogra...            3  \n",
       "118     A positively thrilling combination of ethnogra...            3  \n",
       "131           the intrigue , betrayal , deceit and murder            2  \n",
       "135                          betrayal , deceit and murder            2  \n",
       "137                                   , deceit and murder            2  \n",
       "138                                     deceit and murder            2  \n",
       "145                               a Shakespearean tragedy            2  \n",
       "150                                    a juicy soap opera            1  \n",
       "...                                                   ...          ...  \n",
       "155992             to go with this claustrophobic concept            0  \n",
       "155993                go with this claustrophobic concept            0  \n",
       "155994                   with this claustrophobic concept            0  \n",
       "155995                        this claustrophobic concept            0  \n",
       "155996                             claustrophobic concept            0  \n",
       "155997  Despite these annoyances NOT_, the capable Cla...            3  \n",
       "155999                                  these annoyances             2  \n",
       "156005                              the capable Clayburgh            2  \n",
       "156006                                  capable Clayburgh            2  \n",
       "156007  really do a great job of anchoring the charact...            3  \n",
       "156010  a great job of anchoring the characters in the...            3  \n",
       "156014  anchoring the characters in the emotional real...            2  \n",
       "156023                                  -LRB- Tries -RRB-            2  \n",
       "156028           's already a joke in the United States .            2  \n",
       "156029             's already a joke in the United States            2  \n",
       "156032                              The movie 's downfall            2  \n",
       "156035                 to substitute plot for personality            1  \n",
       "156039  The film is darkly atmospheric , with Herrmann...            3  \n",
       "156040  is darkly atmospheric , with Herrmann quietly ...            3  \n",
       "156041  is darkly atmospheric , with Herrmann quietly ...            3  \n",
       "156043                              is darkly atmospheric            2  \n",
       "156044  with Herrmann quietly suggesting the sadness a...            3  \n",
       "156045  Herrmann quietly suggesting the sadness and ob...            3  \n",
       "156047  quietly suggesting the sadness and obsession b...            3  \n",
       "156048  suggesting the sadness and obsession beneath H...            3  \n",
       "156051                              sadness and obsession            2  \n",
       "156052                                        sadness and            2  \n",
       "156053        beneath Hearst 's forced avuncular chortles            1  \n",
       "156054                Hearst 's forced avuncular chortles            1  \n",
       "156057                                 avuncular chortles            2  \n",
       "\n",
       "[68778 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.Sentiment != train.Predictions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
