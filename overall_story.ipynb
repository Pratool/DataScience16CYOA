{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pattern.en",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-72d8821940bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperceptron\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named pattern.en"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import pattern.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t')\n",
    "test = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extracted only single words from the training data and saved their sentiment into a dictionary. From our data exploration, we noticed that entire sentences could consist of mostly neutral words, but one key positive or negative term will completely change the sentiment of the sentence. So, for each test sentence, we calculated the sentiment by extracting all the words and just using the sentiment of the word with the most extreme sentiment. It's also worth noting that all of the words in the test data are in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracts one-word phrases using a regular expression\n",
    "single_words = train[~train.Phrase.str.contains(' ')]\n",
    "single_words = single_words[single_words.Phrase.str.contains('^[a-zA-Z]+$')]\n",
    "single_words.Phrase = single_words.Phrase.str.lower()\n",
    "\n",
    "# Creates a dictionary mapping words -> sentiment\n",
    "phrase_iter =single_words.Phrase.values\n",
    "sent_iter = single_words.Sentiment.values\n",
    "corpus = dict(zip(phrase_iter, sent_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepend_NOT(match):\n",
    "    \"\"\"\n",
    "    A function that feeds into a regular expression substitution function\n",
    "    that prepends all words after a negation word (i.e. \"didn't\" and\n",
    "    \"not\") with \"NOT_\".\n",
    "    \"\"\"\n",
    "    match = match.group()\n",
    "    words = match.split(\" \")\n",
    "    negation = words[0]\n",
    "    del words[0]\n",
    "    new_words = [\"NOT_\" + word for word in words]\n",
    "    return negation + \" \" + \" \".join(new_words)\n",
    "\n",
    "\n",
    "def substitute_negations(phrase):\n",
    "    \"\"\"\n",
    "    Replaces input phrase with the same phrase, except prepending a \"NOT_\"\n",
    "    for every word after a negation word (i.e. \"didn't\" and \"not\"). This\n",
    "    can only occur in phrases with more than one word.\n",
    "    \"\"\"\n",
    "    # negation_words is a list of regular expressions\n",
    "    negation_words = [r\"not\", r\"n't\", r\"no\"]\n",
    "    \n",
    "    # negation_words then gets turned into a regular expression string\n",
    "    negation_words = [r\"(\" + word + r\")\" for word in negation_words]\n",
    "    negation_words = (r\"|\").join(negation_words)\n",
    "    \n",
    "    negations_re = re.compile(r\"(\" + negation_words + r\")[A-z ']*\")\n",
    "    substitution = negations_re.sub(prepend_NOT, phrase)\n",
    "    \n",
    "    if substitution == \"\":\n",
    "        return phrase\n",
    "    return substitution\n",
    "\n",
    "\n",
    "def add_NOT_to_negations(df):\n",
    "    \"\"\"\n",
    "    Replaces each phrase in the dataframe with the same phrase, but\n",
    "    replacing every word after a negation word (i.e. \"didn't\" and \"not\")\n",
    "    with \"NOT_\" prepended to the word. This can only occur in phrases\n",
    "    with more than one word.\n",
    "    \"\"\"\n",
    "    data = df\n",
    "    data[\"Negations\"] = data[\"Phrase\"].apply(lambda x: substitute_negations(x))\n",
    "    return data\n",
    "\n",
    "train = add_NOT_to_negations(train)\n",
    "test = add_NOT_to_negations(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not good = negative sentiment: 1\n",
      "not bad = positive sentiment: 4\n"
     ]
    }
   ],
   "source": [
    "# Generates a corpus of all the negated words\n",
    "not_corpus = {}\n",
    "for w in corpus:\n",
    "    words = w.split()\n",
    "    if len(words) == 1:\n",
    "        not_corpus[\"NOT_\" + w] = abs(4 - corpus[w])\n",
    "    elif len(words) == 2:\n",
    "        not_corpus[\"NOT_\" + words[0] + \" NOT_\" + words[1]] = abs(4 - corpus[w])\n",
    "    \n",
    "print \"not good = negative sentiment: \" + str(not_corpus[\"NOT_good\"])\n",
    "print \"not bad = positive sentiment: \" + str(not_corpus[\"NOT_bad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr{Sentiment 0}: 0.0453\n",
      "Pr{Sentiment 1}: 0.1747\n",
      "Pr{Sentiment 2}: 0.5099\n",
      "Pr{Sentiment 3}: 0.2109\n",
      "Pr{Sentiment 4}: 0.0589\n"
     ]
    }
   ],
   "source": [
    "# Calculates the probability of any given phrase having a particular sentiment\n",
    "P_c = []\n",
    "for i in range(5):\n",
    "    prob = len(train[train.Sentiment == i]) / float(len(train))\n",
    "    P_c.append(prob)\n",
    "    \n",
    "print \"Pr{Sentiment 0}: \" + str(P_c[0])[:6]\n",
    "print \"Pr{Sentiment 1}: \" + str(P_c[1])[:6]\n",
    "print \"Pr{Sentiment 2}: \" + str(P_c[2])[:6]\n",
    "print \"Pr{Sentiment 3}: \" + str(P_c[3])[:6]\n",
    "print \"Pr{Sentiment 4}: \" + str(P_c[4])[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many times does the word \"exciting\" appear in sentiment 4 phrases?\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Extracts just the sentences from the dataset and combines them into mega-strings\n",
    "sentences_dict = train['SentenceId'].drop_duplicates()\n",
    "sentences = train.iloc[sentences_dict.keys()]\n",
    "sentences.head()\n",
    "\n",
    "# For each sentiment's mega-string, counts the number of times each word appears\n",
    "text = []\n",
    "for i in range(5):\n",
    "    d = {}\n",
    "    temp = sentences[sentences.Sentiment == i]\n",
    "    words_in_class = ' '.join(temp.Phrase.values).lower().split()\n",
    "    for w in words_in_class:\n",
    "        new_word = w.lower().strip('.,;!?\"()')\n",
    "        if not new_word.strip(string.ascii_letters) and new_word != '':\n",
    "            d[new_word] = d.get(new_word,0) + 1\n",
    "\n",
    "    text.append(d)\n",
    "\n",
    "print \"How many times does the word \\\"exciting\\\" appear in sentiment 4 phrases?\"\n",
    "print text[4]['exciting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the word \"yellow\" appearing in a sentiment 2 phrase: \n",
      "5.01957634776e-05\n"
     ]
    }
   ],
   "source": [
    "# Calculates the probability of a word being in a particular class for every word in the corpus\n",
    "P_wc = []\n",
    "alpha = 1.0 # Laplace smoothing\n",
    "for d in text:\n",
    "    n_words_in_class = sum(d.values())\n",
    "    new_dict = {}\n",
    "    for w in corpus:\n",
    "        if w in d:\n",
    "            new_dict[w] = (d[w] + alpha) / (n_words_in_class + alpha*len(corpus))\n",
    "        else:\n",
    "            new_dict[w] = alpha / len(corpus)\n",
    "    for w in not_corpus:\n",
    "        if w in d:\n",
    "            new_dict[w] = (d[w] + alpha) / (n_words_in_class + alpha*len(corpus))\n",
    "        else:\n",
    "            new_dict[w] = alpha / len(corpus)\n",
    "    P_wc.append(new_dict)\n",
    "    \n",
    "print \"Probability of the word \\\"yellow\\\" appearing in a sentiment 2 phrase: \"\n",
    "print P_wc[2]['yellow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the right probability values have been calculated, the Naive Bayes method must first extract all of the words in the phrase. For each sentiment class, the algorithm gets $P(w\\:|\\:c)$ for each word in the phrase and multiplies all of the probabilities together. This product then gets multiplied by $P(c)$. The overall equation looks like the following:\n",
    "\n",
    "$c = \\max_\\limits{c\\:\\in\\:C}\\:\\Big( P(c)\\: \\cdot \\prod_\\limits{x\\:\\in\\:X} P(x\\:|\\:c)\\Big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Splits the phrase into a list of words '''\n",
    "def get_words(phrase):\n",
    "    words = phrase.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(w.lower().strip('.,;!?\"()'))\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "''' Calculates the sentiment of a phrase using a Naive bayes classifier '''\n",
    "def naive_bayes(phrase):\n",
    "    if phrase in corpus:\n",
    "        return corpus[phrase]\n",
    "    if phrase in not_corpus:\n",
    "        return not_corpus[phrase]\n",
    "    \n",
    "    P_cw = 0 # Probability that the phrase has a sentiment c\n",
    "    words = get_words(phrase)\n",
    "    \n",
    "    for s in range(5):\n",
    "        # Probability of the sentiment occurring\n",
    "        prob = P_c[s]\n",
    "        \n",
    "        # Probability of the word appearing in the sentiment class\n",
    "        for w in words:\n",
    "            if w in corpus or w in not_corpus:\n",
    "                prob *= P_wc[s][w]\n",
    "            \n",
    "        if prob > P_cw:\n",
    "            P_cw = prob\n",
    "            sentiment = s\n",
    "            \n",
    "    if sentiment == 2:\n",
    "        pattern_sent = pattern.en.sentiment(phrase)[0]\n",
    "        if abs(pattern_sent) > 0.1:\n",
    "            sentiment = int(round(pattern_sent * 2)) + sentiment\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the Naive Bayes classifier to the training data, we got about the same accuracy as for the first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.54651416122\n"
     ]
    }
   ],
   "source": [
    "train[\"Predictions\"] = train.Phrase.apply(naive_bayes)\n",
    "print \"Train accuracy: \" + str(len(train[train.Sentiment == train.Predictions]) / float(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates the csv file\n",
    "predictions = test.Phrase.apply(naive_bayes)\n",
    "submission = pd.DataFrame({\n",
    "        \"PhraseId\": test[\"PhraseId\"],\n",
    "        \"Sentiment\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"final_iteration.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score we got on Kaggle for this submission was 52.682%, which is slightly lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK's tagging package, we created a new Pandas series that stores the part of speech of each word in the sentence. Though we weren't able to to do much with that data, we think it will come in handy for a future iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the tagger for faster tagging\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def tag_all_phrases(df):\n",
    "    data = df\n",
    "    data[\"POS\"] = data[\"Phrase\"].apply(\n",
    "        lambda x: [tag[1] for tag in \\\n",
    "                   nltk.tag._pos_tag(nltk.word_tokenize(x), None, tagger)] )\n",
    "    return data\n",
    "\n",
    "def get_words(df):\n",
    "    data = df[~(df['Phrase'].str.contains(' '))]\n",
    "    return data\n",
    "\n",
    "def extract_single_POS_from_words(df):\n",
    "    data = df\n",
    "    data[\"POS2\"] = data[\"POS\"].apply(lambda x: x[0])\n",
    "    return data\n",
    "\n",
    "def make_phrase_POS_columns(data, parts_of_speech):\n",
    "    df = data\n",
    "    for POS in parts_of_speech:\n",
    "        df[POS] = [np.asarray([1 if p==POS else 0 for p in POS_list], dtype=int)\\\n",
    "                   for POS_list in df['POS']]\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
