{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t')\n",
    "test = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before designing and implementing a model, we first cleaned the input of any punctuation marks or extra spaces that we did not want to be in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooj/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Removes any extraneous punctuation marks from the phrase \"\"\"\n",
    "def stem_phrases(data):\n",
    "    df = data\n",
    "    \n",
    "    # looks for any commas, periods, and apostrophes with any leading or following spaces\n",
    "    remove_punct = r'(( )*((\\.)|(\\'s*)|(,))( )*)'\n",
    "\n",
    "    # remove leading and trailing spaces\n",
    "    remove_spaces = r'(^( )*)|(( )*$)'\n",
    "\n",
    "    # replaces the pattern mentioned above with a single space\n",
    "    df['Phrase'] = df['Phrase'].str.replace(remove_punct, ' ')\n",
    "    df = df[df['Phrase'] != ' ']\n",
    "    df['Phrase'] = df['Phrase'].str.replace(remove_spaces, '')\n",
    "\n",
    "    # Removes empty string phrases from the list\n",
    "    B = []\n",
    "    before = \"\"\n",
    "    for r in df['Phrase']:\n",
    "        if r != before:\n",
    "            B.append(True)\n",
    "            before = r\n",
    "        else:\n",
    "            B.append(False)\n",
    "    df = df[B]\n",
    "    \n",
    "    return df\n",
    "\n",
    "stemmed = stem_phrases(train)\n",
    "stemmed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extracted only single words from the training data and saved their sentiment into a dictionary. From our data exploration, we noticed that entire sentences could consist of mostly neutral words, but one key positive or negative term will completely change the sentiment of the sentence. So, for each test sentence, we calculated the sentiment by extracting all the words and just using the sentiment of the word with the most extreme sentiment. It's also worth noting that all of the words in the test data are in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracts one-word phrases using a regular expression\n",
    "regexp = re.compile(' ')\n",
    "single_words = train[~train.Phrase.str.contains(' ')]\n",
    "single_words = single_words[single_words.Phrase.str.contains('^[a-zA-Z]+$')]\n",
    "single_words.Phrase = single_words.Phrase.str.lower()\n",
    "\n",
    "# Creates a dictionary mapping words -> sentiment\n",
    "phrase_iter = single_words.Phrase.values\n",
    "sent_iter = single_words.Sentiment.values\n",
    "corpus = dict(zip(phrase_iter, sent_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code finds the word with the most extreme sentiment and sets the entire sentence's sentiment to be equal to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    4\n",
       "2    2\n",
       "3    4\n",
       "4    4\n",
       "Name: Phrase, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = test[:1]\n",
    "\n",
    "''' Splits the phrase into a list of words '''\n",
    "def get_words(phrase):\n",
    "    words = phrase.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(w.lower().strip('.,;!?\"()'))\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "''' Gets the most extreme sentiment value in the sentence and returns it '''\n",
    "def get_sentiment(phrase):\n",
    "    words = get_words(phrase)\n",
    "    most_sentiment = 2\n",
    "    \n",
    "    for w in words:\n",
    "        if w in corpus:\n",
    "            sentiment = corpus[w]\n",
    "            if abs(sentiment - 2) >= abs(most_sentiment - 2):\n",
    "                most_sentiment = sentiment\n",
    "        \n",
    "    return most_sentiment\n",
    "\n",
    "# Predicts the first 5 sentiment values\n",
    "test.Phrase.apply(get_sentiment)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying this to the training data, we actually got a relatively high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.53044982699\n"
     ]
    }
   ],
   "source": [
    "train_predictions = train.Phrase.apply(get_sentiment)\n",
    "print \"Train accuracy: \" + str(len(train[train.Sentiment == train_predictions]) / float(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates the csv file\n",
    "predictions = test.Phrase.apply(get_sentiment)\n",
    "submission = pd.DataFrame({\n",
    "        \"PhraseId\": test[\"PhraseId\"],\n",
    "        \"Sentiment\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"submission_iteration1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, we achieved an accuracy on Kaggle of 54.682%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to improve our model, we tried applying the Naive Bayes model to the data. In order to calculate $P(c\\:|\\:w)$, the probability of the words belonging to a particular sentiment class, we calculated $P(c)$, the probability of each sentiment class occurring in the data, and $P(w\\: |\\: c)$, the probability of a particular word appearing in a sentiment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr{Sentiment 0}: 0.0453\n",
      "Pr{Sentiment 1}: 0.1747\n",
      "Pr{Sentiment 2}: 0.5099\n",
      "Pr{Sentiment 3}: 0.2109\n",
      "Pr{Sentiment 4}: 0.0589\n"
     ]
    }
   ],
   "source": [
    "# Calculates the probability of any given phrase having a particular sentiment\n",
    "P_c = []\n",
    "for i in range(5):\n",
    "    prob = len(train[train.Sentiment == i]) / float(len(train))\n",
    "    P_c.append(prob)\n",
    "    \n",
    "print \"Pr{Sentiment 0}: \" + str(P_c[0])[:6]\n",
    "print \"Pr{Sentiment 1}: \" + str(P_c[1])[:6]\n",
    "print \"Pr{Sentiment 2}: \" + str(P_c[2])[:6]\n",
    "print \"Pr{Sentiment 3}: \" + str(P_c[3])[:6]\n",
    "print \"Pr{Sentiment 4}: \" + str(P_c[4])[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many times does the word \"exciting\" appear in sentiment 4 phrases?\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Extracts just the sentences from the dataset and combines them into mega-strings\n",
    "sentences_dict = train['SentenceId'].drop_duplicates()\n",
    "sentences = train.iloc[sentences_dict.keys()]\n",
    "sentences.head()\n",
    "\n",
    "# For each sentiment's mega-string, counts the number of times each word appears\n",
    "text = []\n",
    "for i in range(5):\n",
    "    d = {}\n",
    "    temp = sentences[sentences.Sentiment == i]\n",
    "    words_in_class = ' '.join(temp.Phrase.values).lower().split()\n",
    "    for w in words_in_class:\n",
    "        new_word = w.lower().strip('.,;!?\"()')\n",
    "        if not new_word.strip(string.ascii_letters) and new_word != '':\n",
    "            d[new_word] = d.get(new_word,0) + 1\n",
    "\n",
    "    text.append(d)\n",
    "\n",
    "print \"How many times does the word \\\"exciting\\\" appear in sentiment 4 phrases?\"\n",
    "print text[4]['exciting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dc990d7e6155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mP_wc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;31m# Laplace smoothing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mn_words_in_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnew_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculates the probability of a word being in a particular class for every word in the corpus\n",
    "P_wc = []\n",
    "alpha = 1.0 # Laplace smoothing\n",
    "for d in text:\n",
    "    n_words_in_class = sum(d.values())\n",
    "    new_dict = {}\n",
    "    for w in corpus:\n",
    "        if w in d:\n",
    "            new_dict[w] = (d[w] + alpha) / (n_words_in_class + len(corpus))\n",
    "        else:\n",
    "            new_dict[w] = alpha / len(corpus)\n",
    "    P_wc.append(new_dict)\n",
    "    \n",
    "print \"Probability of the word \\\"yellow\\\" appearing in a sentiment 2 phrase: \"\n",
    "print P_wc[2]['yellow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the right probability values have been calculated, the Naive Bayes method must first extract all of the words in the phrase. For each sentiment class, the algorithm gets $P(w\\:|\\:c)$ for each word in the phrase and multiplies all of the probabilities together. This product then gets multiplied by $P(c)$. The overall equation looks like the following:\n",
    "\n",
    "$c = \\max_\\limits{c\\:\\in\\:C}\\:\\Big( P(c)\\: \\cdot \\prod_\\limits{x\\:\\in\\:X} P(x\\:|\\:c)\\Big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Calculates the sentiment of a phrase using a Naive bayes classifier '''\n",
    "def naive_bayes(phrase):\n",
    "    words = get_words(phrase)\n",
    "    P_cw = 0 # Probability that the phrase has a sentiment c \n",
    "    \n",
    "    for s in range(5):\n",
    "        # Probability of the sentiment occurring\n",
    "        prob = P_c[s]\n",
    "        \n",
    "        # Probability of the word appearing in the sentiment class\n",
    "        for w in words:\n",
    "            if w in corpus:\n",
    "                prob *= P_wc[s][w]\n",
    "            \n",
    "        if prob > P_cw:\n",
    "            P_cw = prob\n",
    "            sentiment = s\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the Naive Bayes classifier to the training data, we got about the same accuracy as for the first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.522209406638\n"
     ]
    }
   ],
   "source": [
    "train_predictions = train.Phrase.apply(naive_bayes)\n",
    "print \"Train accuracy: \" + str(len(train[train.Sentiment == train_predictions]) / float(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates the csv file\n",
    "predictions = test.Phrase.apply(naive_bayes)\n",
    "submission = pd.DataFrame({\n",
    "        \"PhraseId\": test[\"PhraseId\"],\n",
    "        \"Sentiment\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"submission_iteration2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score we got on Kaggle for this submission was 52.682%, which is slightly lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK's tagging package, we created a new Pandas series that stores the part of speech of each word in the sentence. Though we weren't able to to do much with that data, we think it will come in handy for a future iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooj/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(A, DT), (series, NN), (of, IN), (escapades, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet introspective and entertaining inde...</td>\n",
       "      <td>4</td>\n",
       "      <td>[(This, DT), (quiet, JJ), (introspective, JJ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant work I suspect wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Even, RB), (fans, NNS), (of, IN), (Ismail, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "      <td>[(A, DT), (positively, RB), (thrilling, VBG), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Aggressive, JJ), (self-glorification, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>167</td>\n",
       "      <td>6</td>\n",
       "      <td>A comedy-drama of nearly epic proportions root...</td>\n",
       "      <td>4</td>\n",
       "      <td>[(A, DT), (comedy-drama, NN), (of, IN), (nearl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>7</td>\n",
       "      <td>Narratively Trouble Every Day is a plodding mess</td>\n",
       "      <td>1</td>\n",
       "      <td>[(Narratively, RB), (Trouble, JJ), (Every, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>8</td>\n",
       "      <td>The Importance of Being Earnest so thick with ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[(The, DT), (Importance, NN), (of, IN), (Being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>248</td>\n",
       "      <td>9</td>\n",
       "      <td>But it does n t leave you with much</td>\n",
       "      <td>1</td>\n",
       "      <td>[(But, CC), (it, PRP), (does, VBZ), (n, JJ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>10</td>\n",
       "      <td>You could hate it for the same reason</td>\n",
       "      <td>1</td>\n",
       "      <td>[(You, PRP), (could, MD), (hate, VB), (it, PRP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId  SentenceId                                             Phrase  \\\n",
       "0           1           1  A series of escapades demonstrating the adage ...   \n",
       "63         64           2  This quiet introspective and entertaining inde...   \n",
       "81         82           3  Even fans of Ismail Merchant work I suspect wo...   \n",
       "116       117           4  A positively thrilling combination of ethnogra...   \n",
       "156       157           5  Aggressive self-glorification and a manipulati...   \n",
       "166       167           6  A comedy-drama of nearly epic proportions root...   \n",
       "198       199           7  Narratively Trouble Every Day is a plodding mess    \n",
       "213       214           8  The Importance of Being Earnest so thick with ...   \n",
       "247       248           9               But it does n t leave you with much    \n",
       "259       260          10             You could hate it for the same reason    \n",
       "\n",
       "     Sentiment                                                POS  \n",
       "0            1  [(A, DT), (series, NN), (of, IN), (escapades, ...  \n",
       "63           4  [(This, DT), (quiet, JJ), (introspective, JJ),...  \n",
       "81           1  [(Even, RB), (fans, NNS), (of, IN), (Ismail, N...  \n",
       "116          3  [(A, DT), (positively, RB), (thrilling, VBG), ...  \n",
       "156          1  [(Aggressive, JJ), (self-glorification, NN), (...  \n",
       "166          4  [(A, DT), (comedy-drama, NN), (of, IN), (nearl...  \n",
       "198          1  [(Narratively, RB), (Trouble, JJ), (Every, NNP...  \n",
       "213          3  [(The, DT), (Importance, NN), (of, IN), (Being...  \n",
       "247          1  [(But, CC), (it, PRP), (does, VBZ), (n, JJ), (...  \n",
       "259          1  [(You, PRP), (could, MD), (hate, VB), (it, PRP...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tagger for faster tagging\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "\"\"\" Classifies each word in a phrase by part of speech \"\"\"\n",
    "def tag_sentences(df):\n",
    "    data = df\n",
    "    data[\"POS\"] = data[\"Phrase\"].apply(lambda x: nltk.tag._pos_tag(nltk.word_tokenize(x), None, tagger))\n",
    "    return data\n",
    "\n",
    "def tag_words(words, sentences):\n",
    "    possible_tags = ['CC', 'CD', 'DT', 'EX',\n",
    "                     'FW', 'IN', 'JJ', 'JJR',\n",
    "                     'JJS', 'LS', 'MD', 'NN',\n",
    "                     'NNP', 'NNPS', 'NNS', 'PDT',\n",
    "                     'POS', 'PRP', 'PRP$', 'RB',\n",
    "                     'RBR', 'RBS', 'RP', 'SYM',\n",
    "                     'TO', 'UH', 'VB', 'VBD',\n",
    "                     'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "                     'WDT', 'WDT', 'WP', 'WP$',\n",
    "                     'WRB']\n",
    "    data = []\n",
    "    words_with_pos = words\n",
    "    j = 0\n",
    "    for sentence_id, sentence_pos in zip(sentences[\"SentenceId\"], sentences[\"POS\"]):\n",
    "        sentence_words = words[words[\"SentenceId\"] == sentence_id]\n",
    "        for sentence_word_pos in sentence_pos:\n",
    "            data.append(sentence_word_pos[1])\n",
    "            \n",
    "    for m,n in zip(words_with_pos.Phrase, data):\n",
    "        if j < 15:\n",
    "            print m, n\n",
    "            j += 1\n",
    "    words_with_pos[\"POS\"] = data\n",
    "    return words_with_pos\n",
    "\n",
    "tagged_sentences = tag_sentences(sentences)\n",
    "tagged_sentences.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
